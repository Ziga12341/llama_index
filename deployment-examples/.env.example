# ========================================
# LlamaIndex API Server Configuration
# ========================================

# Copy this file to .env and fill in your values
# NEVER commit .env to version control!

# ========================================
# Environment
# ========================================
ENVIRONMENT=development  # development, staging, production

# ========================================
# API Keys
# ========================================

# LLM Provider: "openrouter" or "openai"
LLM_PROVIDER=openrouter

# OpenRouter API Key (Recommended - access to 200+ models)
# Get your key at https://openrouter.ai/keys
OPENROUTER_API_KEY=sk-or-v1-your-openrouter-api-key-here

# OpenAI API Key (for embeddings, or if using LLM_PROVIDER=openai)
# Note: Even with OpenRouter, you may want OpenAI for embeddings
OPENAI_API_KEY=sk-your-openai-api-key-here

# LlamaParse for advanced PDF parsing (tables, forms, images with vision)
# Get your key at https://cloud.llamaindex.ai/ (Free tier: 1000 pages/day)
LLAMA_CLOUD_API_KEY=llx-your-llamaparse-key-here
USE_LLAMA_PARSE=true

# Optional: Other LLM providers
# ANTHROPIC_API_KEY=sk-ant-your-key
# HUGGINGFACE_API_KEY=hf_your-key
# COHERE_API_KEY=your-cohere-key

# ========================================
# LLM Configuration
# ========================================

# OpenRouter Models (Popular choices):
# - openai/gpt-4-turbo (Fast, excellent quality)
# - openai/gpt-4o (Vision support, latest OpenAI)
# - anthropic/claude-3.5-sonnet (Excellent reasoning)
# - meta-llama/llama-3.1-70b-instruct (Open source, fast)
# - google/gemini-pro-1.5 (Great for long context)
# - mistralai/mixtral-8x7b-instruct (Fast, cost-effective)
#
# OpenAI Models (if LLM_PROVIDER=openai):
# - gpt-4-turbo, gpt-4, gpt-3.5-turbo
#
# See all models: https://openrouter.ai/models
LLM_MODEL=openai/gpt-4-turbo
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=512

# Embedding model (uses OpenAI embeddings - excellent quality)
EMBEDDING_MODEL=text-embedding-3-small

# ========================================
# Indexing Configuration
# ========================================
CHUNK_SIZE=1024
CHUNK_OVERLAP=20
SIMILARITY_TOP_K=5

# ========================================
# Vector Store Configuration
# ========================================
VECTOR_STORE_TYPE=chroma  # chroma, pinecone, qdrant, postgres, milvus, or leave empty for in-memory

# Chroma settings (if using Chroma)
CHROMA_HOST=chroma
CHROMA_PORT=8000
CHROMA_TELEMETRY=FALSE

# Pinecone settings (if using Pinecone)
# PINECONE_API_KEY=your-pinecone-key
# PINECONE_ENVIRONMENT=us-west1-gcp
# PINECONE_INDEX_NAME=llamaindex

# Qdrant settings (if using Qdrant)
# QDRANT_URL=http://localhost:6333
# QDRANT_API_KEY=your-qdrant-key
# QDRANT_COLLECTION_NAME=llamaindex

# ========================================
# Database Configuration (Optional)
# ========================================
DB_USER=llamaindex
DB_PASSWORD=llamaindex_password
DB_NAME=llamaindex
DB_HOST=postgres  # Use 'postgres' when connecting from within Docker, 'localhost' from host machine
DB_PORT=5432      # Internal container port (use 5433 when connecting from host machine)

# Full database URL (alternative to individual settings)
# For connecting from host machine (e.g., pgAdmin):
# DATABASE_URL=postgresql://llamaindex:llamaindex_password@localhost:5433/llamaindex
# For connecting from within Docker containers:
# DATABASE_URL=postgresql://llamaindex:llamaindex_password@postgres:5432/llamaindex

# ========================================
# Redis Configuration (Optional)
# ========================================
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_PASSWORD=
# REDIS_URL=redis://localhost:6379

# ========================================
# Server Configuration
# ========================================
HOST=0.0.0.0
PORT=8000
WORKERS=1  # Number of worker processes (for production, use CPU count)
LOG_LEVEL=info  # debug, info, warning, error, critical

# ========================================
# Storage Configuration
# ========================================
DATA_DIR=./data
STORAGE_DIR=./storage

# ========================================
# Security (Production)
# ========================================
# API_TOKEN=your-secret-api-token-here
# ALLOWED_ORIGINS=https://yourdomain.com,https://app.yourdomain.com
# MAX_UPLOAD_SIZE=10485760  # 10MB in bytes

# ========================================
# Monitoring & Observability (Optional)
# ========================================
# SENTRY_DSN=your-sentry-dsn
# PROMETHEUS_ENABLED=true
# PROMETHEUS_PORT=9090

# ========================================
# Rate Limiting (Optional)
# ========================================
# RATE_LIMIT_ENABLED=true
# RATE_LIMIT_PER_MINUTE=60

# ========================================
# Cloud Provider Credentials (if needed)
# ========================================
# AWS_ACCESS_KEY_ID=your-aws-key
# AWS_SECRET_ACCESS_KEY=your-aws-secret
# AWS_REGION=us-east-1

# GCP_PROJECT_ID=your-gcp-project
# GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json

# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
# AZURE_OPENAI_API_KEY=your-azure-key
# AZURE_OPENAI_API_VERSION=2023-05-15
